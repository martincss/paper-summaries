#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extbook
\begin_preamble
%\usepackage[style=numeric,backend=biber,maxbibnames=99]{biblatex}
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "utopia" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex-natbib
\cite_engine_type authoryear
\biblio_style plain
\biblio_options maxbibnames=99
\biblatex_bibstyle authoryear
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 6 -1
\bullet 2 0 7 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Paper summaries
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vec}[1]{\text{\textbf{#1}}}
\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset href
LatexCommand href
name "On the Automatic Generation of Medical Imaging Reports"
target "https://docs.google.com/presentation/d/1m4Z3RJDueaKMvCO-cb_Ia51nTW6-OSE9hXVPy1l1-_I/edit?usp=sharing"
literal "false"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "baoyu_jing2018"
literal "false"

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The reading and interpretation of medical images are usually conducted by
 specialized medical professionals.
 Report writing can be error-prone for inexperienced physicians, and time-consum
ing and tedious for experienced physicians.
 
\begin_inset Newline newline
\end_inset

Several challenges need to be addressed:
\end_layout

\begin_layout Enumerate
A complete report consists of multiple heterogeneous sources of information
\end_layout

\begin_layout Enumerate
Localize image regions and attach the right description to them 
\end_layout

\begin_layout Enumerate
Descriptions in reports are usually long, with multiple sentences
\end_layout

\begin_layout Standard
The proposed solutions are:
\end_layout

\begin_layout Enumerate
A 
\series bold
multi-task learning framework
\series default
 for simultaneous prediction of tags and text generation
\end_layout

\begin_layout Enumerate
A 
\series bold
Co-attention mechanism: 
\series default
simultaneous attention to images and predicted tags; explores synergistic
 effects of visual and semantic information 
\end_layout

\begin_layout Enumerate
A 
\series bold
Hierarchical LSTM: 
\series default
Leverages compositional nature of reports: first generates high-level topics,
 then fine-grained descriptions from each one
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/baoyu_jing_report.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sample report from IU X-ray
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods and Architecture
\end_layout

\begin_layout Standard
An image is divided into regions, and a CNN encoder is used to learn visual
 features for these patches.
 These features are fed into a 
\emph on
multi-label classifier
\emph default
, from which tags are predicted.
 These tags are transformed into 
\emph on
semantic feature vectors 
\emph default
by a custom embedding.
 Both visual and semantic features are fed into the co-attention module,
 which produces a combined 
\emph on
context vector, 
\emph default
which 
\series bold
simultaneously captures the visual and semantic information of this image.

\series default
 
\end_layout

\begin_layout Standard
The decoding and caption generation process is performed by the hierarchical
 LSTM, which leverages the compositional structure of a medical report (each
 sentence focusing on one specific topic).
 The 
\emph on
sentence LSTM
\emph default
, using the context vector, first generates a sequence of high-level topic
 vectors representing sentences.
 Each one is passed to the 
\emph on
word LSTM,
\emph default
 which then generates a sentence for each topic vector.
 The number of sentences or topic vectors to be generated is regulated by
 the 
\emph on
stop control
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/baoyu_jing_architecture.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Tag prediction
\end_layout

\begin_layout Standard
This is treated as a multi-label classification task.
 Given an image 
\begin_inset Formula $I$
\end_inset

, visual features 
\begin_inset Formula $\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\in\mathbb{R}^{D}$
\end_inset

 are extracted from the CNN encoder, and fed to a 
\emph on
multi-label classification 
\emph default
(MLC) network, which then generates a probability distribution over the
 
\begin_inset Formula $L$
\end_inset

 tags
\begin_inset Formula 
\[
\boldsymbol{p}_{I,\text{pred}}\left(\boldsymbol{l}_{i}=1\;|\;\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\right)\propto\exp\left(\text{MLC}_{i}\left(\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\right)\right)
\]

\end_inset


\begin_inset Newline newline
\end_inset

where 
\begin_inset Formula $\boldsymbol{l}\in\mathbb{R}^{L}$
\end_inset

 is a binary tag vector, each component representing the presence or absence
 of the corresponding tag.
\begin_inset Newline newline
\end_inset

Finally, the embeddings of the 
\begin_inset Formula $M$
\end_inset

 most likely tags 
\begin_inset Formula $\left\{ \boldsymbol{a}_{m}\right\} _{m=1}^{M}$
\end_inset

 are used as 
\series bold
semantic features.
 
\end_layout

\begin_layout Subsection
Co-Attention
\end_layout

\begin_layout Standard
Visual attention does not provide sufficient high-level semantic information,
 which the tags can always provide.
 A co-attention mechanism can simultaneously attend to visual and semantic
 modalities.
\end_layout

\begin_layout Standard
In the sentence LSTM at time step 
\begin_inset Formula $s$
\end_inset

, the joint context vector 
\series bold

\begin_inset Formula $\text{\textbf{ctx}}^{(s)}\in\mathbb{R}^{C}$
\end_inset

 
\series default
is generated by a co-attention network 
\begin_inset Formula $f_{\text{co-att}}\left(\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N},\left\{ \boldsymbol{a}_{m}\right\} _{m=1}^{M},\boldsymbol{h}_{\text{sent}}^{(s-1)}\right)$
\end_inset

, with 
\begin_inset Formula $\boldsymbol{h}_{\text{sent}}^{(s-1)}\in\mathbb{R}^{H}$
\end_inset

 being the previous hidden state.
 The co-attention network 
\begin_inset Formula $f_{\text{co-att}}$
\end_inset

 uses a single feedforward layer to compute separate soft visual and semantic
 attentions
\begin_inset Formula 
\begin{align*}
\alpha_{\vec v,n} & \propto\exp\left(\vec W_{\vec v_{\text{att}}}\vec v_{n}+\vec W_{\vec v,\vec h}\vec h_{\text{sent}}^{(s-1)}\right)\\
\alpha_{\vec a,m} & \propto\exp\left(\vec W_{\vec a_{\text{att}}}\vec a_{m}+\vec W_{\vec a,\vec h}\vec h_{\text{sent}}^{(s-1)}\right)
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

The visual and semantic context vectors 
\begin_inset Formula 
\begin{align*}
\vec v_{\text{att}}^{(s)} & =\sum_{n=1}^{N}\alpha_{\vec v,n}\vec v_{n}\\
\vec a_{\text{att}}^{(s)} & =\sum_{m=1}^{M}\alpha_{\vec a,m}\vec a_{m}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

These context vector may be combined by concatenation followed by a fully
 connected layer:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\vec{ctx}^{(s)}=\vec W_{\text{fc}}\left[\vec v_{\text{att}}^{(s)};\vec a_{\text{att}}^{(s)}\right]
\]

\end_inset


\end_layout

\begin_layout Subsection
Sentence LSTM
\end_layout

\begin_layout Standard
A single LSTM layer whose input is the joint context vector 
\begin_inset Formula $\vec{ctx}^{(s)}$
\end_inset

 and generates a topic vector 
\begin_inset Formula $\vec t\in\mathbb{R}^{K}$
\end_inset

as long as the stop control allows it to.
 
\end_layout

\begin_layout Paragraph
Topic generator
\end_layout

\begin_layout Standard
Deep output layer (LSTM + multi-layer feedforward) 
\begin_inset Formula 
\[
\vec t^{(s)}=\tanh\left(\vec W_{\vec t,\vec h}\vec h_{\text{sent}}^{(s)}+\vec W_{\vec t,\vec{ctx}}\vec{ctx}^{(s)}\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph
Stop control 
\end_layout

\begin_layout Standard
Deep output layer for the continuation of the sentence LSTM.
 The layer takes the previous and current hidden states and produces a distribut
ion over 
\begin_inset Formula $\left\{ \text{STOP}=1,\text{CONTINUE}=0\right\} $
\end_inset

, 
\begin_inset Formula $p_{\text{stop}}^{(s)}$
\end_inset


\begin_inset Formula 
\begin{align*}
p\left(\text{STOP}\;|\;\vec h_{\text{sent}}^{(s-1)},\vec h_{\text{sent}}^{(s)}\right) & \propto\exp\left\{ \text{\vec W_{\text{stop}}\tanh\left(\vec W_{\text{stop},s-1}\vec h_{\text{sent}}^{(s-1)}+\vec W_{\text{stop},s-1}\vec h_{\text{sent}}^{(s)}\right)}\right\} 
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

This stopping probability is then compared with a predefined threshold.
\end_layout

\begin_layout Subsection
Word LSTM
\end_layout

\begin_layout Standard
For each topic vector, the words in the sentence are generated by the 
\emph on
word LSTM.

\emph default
 Its first and second inputs are the topic vector 
\begin_inset Formula $\vec t$
\end_inset

 and a 
\begin_inset Formula $\text{START}$
\end_inset

 token, then followed by the rest of the words 
\begin_inset CommandInset citation
LatexCommand citep
key "krause2016hierarchical"
literal "false"

\end_inset

.
 Each hidden state 
\begin_inset Formula $\vec h_{\text{word}}\in\mathbb{R}^{H}$
\end_inset

 is directly used to predict the distribution over words:
\begin_inset Formula 
\[
p\left(\text{word}\;|\;\vec h_{\text{word}}\right)\propto\exp\left(\vec W_{\text{out}}\vec h_{\text{word}}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Parameter learning
\end_layout

\begin_layout Standard
Each training example is a tuple 
\begin_inset Formula $\left(I,\text{\vec l,\vec w}\right)$
\end_inset

, with 
\begin_inset Formula $\vec w$
\end_inset

 being the paragraph, with 
\begin_inset Formula $S$
\end_inset

 sentences, each with 
\begin_inset Formula $T_{S}$
\end_inset

 words (ground truth).
 
\end_layout

\begin_layout Enumerate
The MLC predicts the tag distribution 
\begin_inset Formula $\vec p_{I,\text{pred}}$
\end_inset

.
 Its ground truth may be computed by 
\begin_inset Formula $\vec p_{I}=\vec l/\left\Vert \vec l\right\Vert _{1}$
\end_inset

.
 Thus, the training loss for this task is the cross-entropy between both
 distributions 
\begin_inset Formula $\ell_{\text{tag}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
The sentence LSTM is unrolled for 
\begin_inset Formula $S$
\end_inset

 steps, producing that number of topic vectors 
\begin_inset Formula $\vec t^{(s)}$
\end_inset

 and stop distributions 
\begin_inset Formula $p_{\text{stop}}^{(s)}$
\end_inset

.
 For each sentence, this stop probability is compared with the indicator
 
\begin_inset Formula $I\left\{ s=S\right\} $
\end_inset

, which evaluates to 
\begin_inset Formula $0\;\text{[CONTINUE]}$
\end_inset

, until 
\begin_inset Formula $s=S$
\end_inset

, when it evaluates to 
\begin_inset Formula $1\;\text{[STOP]}$
\end_inset

 (that is, a kronecker delta 
\begin_inset Formula $\delta_{sS}$
\end_inset

).
 The cross entropy between them is the loss 
\begin_inset Formula $\ell_{\text{sent}}$
\end_inset

.
\end_layout

\begin_layout Enumerate
The 
\begin_inset Formula $S$
\end_inset

 topic vectors are fed to the word LSTM to generate 
\begin_inset Formula $\vec w_{s,t}$
\end_inset

 words.
 The training loss for each word is the cross entropy 
\begin_inset Formula $\ell_{\text{word}}$
\end_inset

 between the ground truth word 
\begin_inset Formula $w_{s,t}$
\end_inset

 and the predicted word distribution 
\begin_inset Formula $p_{s,t}$
\end_inset

.
\end_layout

\begin_layout Standard
Thus, the overall training loss is 
\begin_inset Formula 
\begin{align*}
\ell\left(I,\text{\vec l},\vec w\right) & =\lambda_{\text{tag}}\ell_{\text{tag}}\\
 & +\lambda_{\text{sent}}\sum_{s=1}^{S}\ell_{\text{sent}}\left(p_{\text{stop}}^{(s)},I\left\{ s=S\right\} \right)\\
 & +\lambda_{\text{word}}\sum_{s=1}^{S}\sum_{t=1}^{T_{S}}\ell_{\text{word}}\left(p_{s,t},w_{s,t}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Furthermore, and attention regularization loss 
\begin_inset CommandInset citation
LatexCommand citep
key "xu2015attend"
literal "false"

\end_inset

 for both visual 
\begin_inset Formula $\boldsymbol{\alpha}\in\mathbb{R}^{N\times S}$
\end_inset

 and semantic 
\begin_inset Formula $\boldsymbol{\beta}\in\mathbb{R}^{M\times S}$
\end_inset

 attention coefficients.
 This regularization encourages the model to pay equal attention over different
 image regions and tags.
 
\begin_inset Formula 
\[
\ell_{\text{reg}}=\lambda_{\text{reg}}\left[\sum_{n=1}^{N}\left(1-\sum_{s=1}^{S}\alpha_{n,s}\right)^{2}+\sum_{m=1}^{M}\left(1-\sum_{s=1}^{S}\beta_{m,s}\right)^{2}\right]
\]

\end_inset


\end_layout

\begin_layout Chapter
Evaluation of text generation: A survey 
\begin_inset CommandInset citation
LatexCommand citep
key "reviewmetrics2020"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Surveys evaluation methods for Natural Language Generation (NLG) that may
 be classified as 
\end_layout

\begin_layout Itemize
Human-centric evaluation
\end_layout

\begin_layout Itemize
Automatic metrics
\end_layout

\begin_layout Itemize
Machine-learned evaluation
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
NLG evaluation is challenging mainly because many NLG tasks are open-ended
 (
\emph on
i.e.
 
\emph default
multiple valid answers for a task), so human evaluation remains the gold
 standard.
 
\end_layout

\begin_layout Standard
NLG techs range from template-based systems to machine learned systems that
 have a complex understanding of human grammar.
 There has been a paradigm shift from earlier template-based models using
 statistical methods and expert knowledge to unsupervised learning of representa
tions from large textual corpora by using DNN models.
 This shift crosses through RNNs, (LSTM, GRU), word2vec, GloVe, and sequence-to-
sequence by encoder-decoder architecture.
 These latter models' weakness of capturing long-range dependencies motivates
 the development of 
\emph on
attention networks
\emph default
 and 
\emph on
pointer networks
\emph default
.
 The transformer architecture combines both encoder-decoder with a self-attentio
n mechanism.
 
\end_layout

\begin_layout Standard
Neural models have been applied to many NLG tasks:
\end_layout

\begin_layout Itemize
summarization: documents, query-focused or generic, for news articles, meetings,
 etc
\end_layout

\begin_layout Itemize
machine translation
\end_layout

\begin_layout Itemize
dialog systems: goal-oriented or chit-chat
\end_layout

\begin_layout Itemize
question generation
\end_layout

\begin_layout Itemize
long text generation: story, news, poem
\end_layout

\begin_layout Itemize
data-to-text generation: 
\emph on
e.g.

\emph default
 table summarization
\end_layout

\begin_layout Itemize

\series bold
caption generation from non-text input:
\series default
 tables, images or sequences of video frames 
\end_layout

\begin_layout Quote

\emph on
\begin_inset Quotes eld
\end_inset

Nevertheless, training a powerful language model relies on evaluation metrics
 that can measure the model quality from different perspectives.
 For instance, it is imperative to build evaluation methods that can determine
 whether a text is generated by a human or a machine to prevent any potential
 harm.
 Similarly, evaluating the generated text based on factual consistency has
 recently drawn attention in the NLG field.
 It is concerning that neural language models can generate open-ended texts
 that are fluent but not grounded in real-world knowledge or facts, such
 as fake news.
 The situation is particularly alarming if the generated reports or news
 are related to the well-being of humankind, such as summaries of health
 reports.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsection
Outline
\end_layout

\begin_layout Itemize

\series bold
Human-centric Evaluation: 
\series default
\emph on
humans as judges.
 
\emph default
Task-specific.
 
\end_layout

\begin_layout Itemize

\series bold
Untrained Automatic Metrics: 
\series default
Most commonly used, compared model outputs to references with metrics based
 on string overlap, content overlap, string distance or lexical diversity.
\end_layout

\begin_layout Itemize

\series bold
Machine-learned Metrics: 
\series default
models that can be viewed as digital judges that simulate human judges.
\end_layout

\begin_layout Section
Human-Centric Evaluation Methods
\end_layout

\begin_layout Standard
The ultimate goal of NLG is to generate text that is valuable to people.
 
\begin_inset Newline newline
\end_inset

Human evaluations pose several challenges: expensive and time-consuming
 to run, especially for tasks that require domain expertise.
 There is also a lack of consistency in how these evaluations are run, which
 prevents reproducibility and comparing across systems.
 
\end_layout

\begin_layout Subsection
Intrinsic Evaluation
\end_layout

\begin_layout Standard
An 
\emph on
intrinsic evaluation
\emph default
 asks people to evaluate the quality of generated text, either overall or
 along some specific dimension (e.g., fluency, coherence, correctness, etc.).
 This may be done by showing each text individually and asking for an assessment
 by binary (good/bad) or Likert/sliding scale.
 However, these judgments can be inconsistent and comparing these results
 is not straightforward.
 
\end_layout

\begin_layout Standard
Another approach to directly compare to baselines model variants, or human
 generated texts: evaluations can be performed by selecting or ranking generated
 texts.
 This captures models' relative quality but not absolute.
 This can be solved by asking judges to indicate how much better the chosen
 text is over alternatives.
 Comparison-based approaches can become prohibitively expensive (by number
 of matchups), though this may be somewhat mitigated.
\end_layout

\begin_layout Standard
Some dimensions of human evaluation include:
\end_layout

\begin_layout Itemize

\emph on
Adequacy: â€œhow much of the meaning expressed in the gold-standard translation
 or source is also expressed in the target translation
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Itemize

\emph on
Fluency: 
\emph default
quality of text without taking the source into account, accounting for grammar,
 spelling, choice of words, style, etc.
\end_layout

\begin_layout Itemize

\emph on
Factuality: 
\emph default
important in tasks that require the generated text to accurately reflect
 facts described in the context (many neural NLG models 
\begin_inset Quotes eld
\end_inset

hallucinate
\begin_inset Quotes erd
\end_inset

 information)
\end_layout

\begin_layout Itemize
Misc: 
\emph on
commonsense, logicality, coherence or consistency
\end_layout

\begin_layout Standard
Other dimensions focus on 
\emph on
how 
\emph default
the text is being said, without context:
\end_layout

\begin_layout Itemize

\emph on
Grammaticality
\end_layout

\begin_layout Itemize

\emph on
Style, formality 
\emph default
or 
\emph on
tone
\end_layout

\begin_layout Itemize

\emph on
Typicality: 
\emph default
how often do you expect to see text that looks like this?
\end_layout

\begin_layout Itemize

\emph on
Redundancy
\end_layout

\begin_layout Subsection
Extrinsic evaluation
\end_layout

\begin_layout Standard
An 
\emph on
extrinsic evaluation 
\emph default
asks people to evaluate the system's performance on the task for which it
 was designed.
 They are the most meaningful for downstream tasks, but can be expensive
 and difficult to run.
 
\end_layout

\begin_layout Subsection
The Evaluators
\end_layout

\begin_layout Standard
For many NLG tasks, no specific expertise is required of the evaluators
 (other than language).
 Many evaluators can be crowdsourced online (
\emph on
e.g.
 
\emph default
Amazon Mechanical Turk), though issues of quality control may be raised.
 Other cases require expert human annotators.
\end_layout

\begin_layout Subsection
Inter-Evaluator Agreement
\end_layout

\begin_layout Standard
Evaluating natural language will always include some degree of subjectivity.
 The rate of evaluator disagreement is also useful to measure, since high
 agreement can signal the task is well-defined and differences in generated
 text are noticeable, while low agreement can mean the opposite.
 
\end_layout

\begin_layout Subsubsection
Percent agreement
\end_layout

\begin_layout Standard
Flat percent of cases in which the evaluators agree: 
\begin_inset Formula $X$
\end_inset

 is the set of generated texts, and 
\begin_inset Formula $a_{i}=1$
\end_inset

 indicates agreement
\begin_inset Formula 
\begin{equation}
P_{a}=\dfrac{1}{\left|X\right|}\sum_{i=0}^{\left|X\right|}a_{i}\label{eq:p_agree}
\end{equation}

\end_inset


\begin_inset Newline newline
\end_inset

This may be a common metric, but does not take into account chance agreement,
 especially with small number of scores (hypothesis testing?)
\end_layout

\begin_layout Subsubsection
Cohen's 
\begin_inset Formula $\kappa$
\end_inset


\end_layout

\begin_layout Standard
This measure can capture agreements that may happen by chance: we now consider
 such probability
\begin_inset Formula $P_{c}$
\end_inset

.
 For 
\series bold
two
\series default
 evaluators are scoring 
\begin_inset Formula $X$
\end_inset

 with a score from a set 
\begin_inset Formula $S$
\end_inset

, the probability of all scoring a text the same (assuming independence):
\begin_inset Formula 
\[
P_{c}=\sum_{s\in S}P\left(s\,|\,e_{1}\right)P\left(s\,|\,e_{2}\right)
\]

\end_inset


\begin_inset Newline newline
\end_inset

Each 
\begin_inset Formula $P\left(s\,|\,e_{i}\right)$
\end_inset

 is estimated by the frequency with which evaluator 
\begin_inset Formula $e_{i}$
\end_inset

 assigned each score 
\begin_inset Formula $s$
\end_inset

.
 Cohen's 
\begin_inset Formula $\kappa$
\end_inset

 is defined as
\begin_inset Formula 
\[
\kappa=\dfrac{P_{a}-P_{c}}{1-P_{c}}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Fleiss' 
\begin_inset Formula $\kappa$
\end_inset


\end_layout

\begin_layout Standard
Generalizes Cohen's 
\begin_inset Formula $\kappa$
\end_inset

 for more than two raters, by considering agreement across all pairs of
 evaluators.
 We now define the agreement 
\begin_inset Formula $a_{i}$
\end_inset

 for each text 
\begin_inset Formula $x_{i}$
\end_inset

 as 
\begin_inset Formula 
\[
a_{i}=\dfrac{\sum_{s\in S}\text{\# of evaluator pairs who score \ensuremath{x_{i}}as \ensuremath{s}}}{\text{total \# of evaluator pairs}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

The agreement probability 
\begin_inset Formula $P_{a}$
\end_inset

 can thus be computed by 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:p_agree"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The chance agreement probability is estimated by aggregating the frequency
 of scores across all annotators, with the assumption that each judge is
 equally likely to draw from this distribution 
\begin_inset Formula $r_{s}$
\end_inset

.
 Thus, the chance of two annotators assigning score 
\begin_inset Formula $s$
\end_inset

 by chance is 
\begin_inset Formula $r_{s}^{2}$
\end_inset

, and the overall probability is 
\begin_inset Formula 
\[
P_{c}=\sum_{s\in S}r_{s}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Fleiss' 
\begin_inset Formula $\kappa$
\end_inset

 is once again defined as 
\begin_inset Formula 
\[
\kappa=\dfrac{P_{a}-P_{c}}{1-P_{c}}
\]

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "papers"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
