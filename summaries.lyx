#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%\usepackage[style=numeric,backend=biber,maxbibnames=99]{biblatex}
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "utopia" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex-natbib
\cite_engine_type authoryear
\biblio_style plain
\biblio_options maxbibnames=99
\biblatex_bibstyle authoryear
\biblatex_citestyle authoryear
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\bullet 1 0 6 -1
\bullet 2 0 7 -1
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Paper summaries
\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vec}[1]{\text{\textbf{#1}}}
\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset href
LatexCommand href
name "On the Automatic Generation of Medical Imaging Reports"
target "https://docs.google.com/presentation/d/1m4Z3RJDueaKMvCO-cb_Ia51nTW6-OSE9hXVPy1l1-_I/edit?usp=sharing"
literal "false"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "baoyu_jing2018"
literal "false"

\end_inset


\end_layout

\begin_layout Subsection
Introduction
\end_layout

\begin_layout Standard
The reading and interpretation of medical images are usually conducted by
 specialized medical professionals.
 Report writing can be error-prone for inexperienced physicians, and time-consum
ing and tedious for experienced physicians.
 
\begin_inset Newline newline
\end_inset

Several challenges need to be addressed:
\end_layout

\begin_layout Enumerate
A complete report consists of multiple heterogeneous sources of information
\end_layout

\begin_layout Enumerate
Localize image regions and attach the right description to them 
\end_layout

\begin_layout Enumerate
Descriptions in reports are usually long, with multiple sentences
\end_layout

\begin_layout Standard
The proposed solutions are:
\end_layout

\begin_layout Enumerate
A 
\series bold
multi-task learning framework
\series default
 for simultaneous prediction of tags and text generation
\end_layout

\begin_layout Enumerate
A 
\series bold
Co-attention mechanism: 
\series default
simultaneous attention to images and predicted tags; explores synergistic
 effects of visual and semantic information 
\end_layout

\begin_layout Enumerate
A 
\series bold
Hierarchical LSTM: 
\series default
Leverages compositional nature of reports: first generates high-level topics,
 then fine-grained descriptions from each one
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/baoyu_jing_report.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sample report from IU X-ray
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Methods and Architecture
\end_layout

\begin_layout Standard
An image is divided into regions, and a CNN encoder is used to learn visual
 features for these patches.
 These features are fed into a 
\emph on
multi-label classifier
\emph default
, from which tags are predicted.
 These tags are transformed into 
\emph on
semantic feature vectors 
\emph default
by a custom embedding.
 Both visual and semantic features are fed into the co-attention module,
 which produces a combined 
\emph on
context vector, 
\emph default
which 
\series bold
simultaneously captures the visual and semantic information of this image.

\series default
 
\end_layout

\begin_layout Standard
The decoding and caption generation process is performed by the hierarchical
 LSTM, which leverages the compositional structure of a medical report (each
 sentence focusing on one specific topic).
 The 
\emph on
sentence LSTM
\emph default
, using the context vector, first generates a sequence of high-level topic
 vectors representing sentences.
 Each one is passed to the 
\emph on
word LSTM,
\emph default
 which then generates a sentence for each topic vector.
 The number of sentences or topic vectors to be generated is regulated by
 the 
\emph on
stop control
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/baoyu_jing_architecture.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Tag prediction
\end_layout

\begin_layout Standard
This is treated as a multi-label classification task.
 Given an image 
\begin_inset Formula $I$
\end_inset

, visual features 
\begin_inset Formula $\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\in\mathbb{R}^{D}$
\end_inset

 are extracted from the CNN encoder, and fed to a 
\emph on
multi-label classification 
\emph default
(MLC) network, which then generates a probability distribution over the
 
\begin_inset Formula $L$
\end_inset

 tags
\begin_inset Formula 
\[
\boldsymbol{p}_{I,\text{pred}}\left(\boldsymbol{l}_{i}=1\;|\;\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\right)\propto\exp\left(\text{MLC}_{i}\left(\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N}\right)\right)
\]

\end_inset


\begin_inset Newline newline
\end_inset

where 
\begin_inset Formula $\boldsymbol{l}\in\mathbb{R}^{L}$
\end_inset

 is a binary tag vector, each component representing the presence or absence
 of the corresponding tag.
\begin_inset Newline newline
\end_inset

Finally, the embeddings of the 
\begin_inset Formula $M$
\end_inset

 most likely tags 
\begin_inset Formula $\left\{ \boldsymbol{a}_{m}\right\} _{m=1}^{M}$
\end_inset

 are used as 
\series bold
semantic features.
 
\end_layout

\begin_layout Subsubsection
Co-Attention
\end_layout

\begin_layout Standard
Visual attention does not provide sufficient high-level semantic information,
 which the tags can always provide.
 A co-attention mechanism can simultaneously attend to visual and semantic
 modalities.
\end_layout

\begin_layout Standard
In the sentence LSTM at time step 
\begin_inset Formula $s$
\end_inset

, the joint context vector 
\series bold

\begin_inset Formula $\text{\textbf{ctx}}^{(s)}\in\mathbb{R}^{C}$
\end_inset

 
\series default
is generated by a co-attention network 
\begin_inset Formula $f_{\text{co-att}}\left(\left\{ \boldsymbol{v}_{n}\right\} _{n=1}^{N},\left\{ \boldsymbol{a}_{m}\right\} _{m=1}^{M},\boldsymbol{h}_{\text{sent}}^{(s-1)}\right)$
\end_inset

, with 
\begin_inset Formula $\boldsymbol{h}_{\text{sent}}^{(s-1)}\in\mathbb{R}^{H}$
\end_inset

 being the previous hidden state.
 The co-attention network 
\begin_inset Formula $f_{\text{co-att}}$
\end_inset

 uses a single feedforward layer to compute separate soft visual and semantic
 attentions
\begin_inset Formula 
\begin{align*}
\alpha_{\vec v,n} & \propto\exp\left(\vec W_{\vec v_{\text{att}}}\vec v_{n}+\vec W_{\vec v,\vec h}\vec h_{\text{sent}}^{(s-1)}\right)\\
\alpha_{\vec a,m} & \propto\exp\left(\vec W_{\vec a_{\text{att}}}\vec a_{m}+\vec W_{\vec a,\vec h}\vec h_{\text{sent}}^{(s-1)}\right)
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

The visual and semantic context vectors 
\begin_inset Formula 
\begin{align*}
\vec v_{\text{att}}^{(s)} & =\sum_{n=1}^{N}\alpha_{\vec v,n}\vec v_{n}\\
\vec a_{\text{att}}^{(s)} & =\sum_{m=1}^{M}\alpha_{\vec a,m}\vec a_{m}
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

These context vector may be combined by concatenation followed by a fully
 connected layer:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\vec{ctx}^{(s)}=\vec W_{\text{fc}}\left[\vec v_{\text{att}}^{(s)};\vec a_{\text{att}}^{(s)}\right]
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Sentence LSTM
\end_layout

\begin_layout Standard
A single LSTM layer whose input is the joint context vector 
\begin_inset Formula $\vec{ctx}^{(s)}$
\end_inset

 and generates a topic vector 
\begin_inset Formula $\vec t\in\mathbb{R}^{K}$
\end_inset

as long as the stop control allows it to.
 
\end_layout

\begin_layout Paragraph
Topic generator
\end_layout

\begin_layout Standard
Deep output layer (LSTM + multi-layer feedforward) 
\begin_inset Formula 
\[
\vec t^{(s)}=\tanh\left(\vec W_{\vec t,\vec h}\vec h_{\text{sent}}^{(s)}+\vec W_{\vec t,\vec{ctx}}\vec{ctx}^{(s)}\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph
Stop control 
\end_layout

\begin_layout Standard
Deep output layer for the continuation of the sentence LSTM.
 The layer takes the previous and current hidden states and produces a distribut
ion over 
\begin_inset Formula $\left\{ \text{STOP}=1,\text{CONTINUE}=0\right\} $
\end_inset


\begin_inset Formula 
\begin{align*}
p\left(\text{STOP}\;|\;\vec h_{\text{sent}}^{(s-1)},\vec h_{\text{sent}}^{(s)}\right) & \propto\\
 & \exp\left\{ \text{\vec W_{\text{stop}}\tanh\left(\vec W_{\text{stop},s-1}\vec h_{\text{sent}}^{(s-1)}+\vec W_{\text{stop},s-1}\vec h_{\text{sent}}^{(s)}\right)}\right\} 
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset

This stopping probability is then compared with a predefined threshold.
\end_layout

\begin_layout Subsubsection
Word LSTM
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "papers"
options "apalike"

\end_inset


\end_layout

\end_body
\end_document
