@misc{xu2015attend,
    title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
    author={Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio},
    year={2015},
    eprint={1502.03044},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{baoyu_jing2018,
    title = "On the Automatic Generation of Medical Imaging Reports",
    author = "Jing, Baoyu  and
      Xie, Pengtao  and
      Xing, Eric",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1240",
    doi = "10.18653/v1/P18-1240",
    pages = "2577--2586",
    abstract = "Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.",
}


@misc{you2016image,
    title={Image Captioning with Semantic Attention},
    author={Quanzeng You and Hailin Jin and Zhaowen Wang and Chen Fang and Jiebo Luo},
    year={2016},
    eprint={1603.03925},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{vinyals2014tell,
    title={Show and Tell: A Neural Image Caption Generator},
    author={Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
    year={2014},
    eprint={1411.4555},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{fang2014captions,
    title={From Captions to Visual Concepts and Back},
    author={Hao Fang and Saurabh Gupta and Forrest Iandola and Rupesh Srivastava and Li Deng and Piotr Dollár and Jianfeng Gao and Xiaodong He and Margaret Mitchell and John C. Platt and C. Lawrence Zitnick and Geoffrey Zweig},
    year={2014},
    eprint={1411.4952},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{karpathy2014deep,
    title={Deep Visual-Semantic Alignments for Generating Image Descriptions},
    author={Andrej Karpathy and Li Fei-Fei},
    year={2014},
    eprint={1412.2306},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{krause2016hierarchical,
    title={A Hierarchical Approach for Generating Descriptive Image Paragraphs},
    author={Jonathan Krause and Justin Johnson and Ranjay Krishna and Li Fei-Fei},
    year={2016},
    eprint={1611.06607},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{liang2017recurrent,
    title={Recurrent Topic-Transition GAN for Visual Paragraph Generation},
    author={Xiaodan Liang and Zhiting Hu and Hao Zhang and Chuang Gan and Eric P. Xing},
    year={2017},
    eprint={1703.07022},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{park-etal-2020-feature-difference,
    title = "{F}eature {D}ifference {M}akes {S}ense: {A} medical image captioning model exploiting feature difference and tag information",
    author = "Park, Hyeryun  and
      Kim, Kyungmo  and
      Yoon, Jooyoung  and
      Park, Seongkeun  and
      Choi, Jinwook",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-srw.14",
    doi = "10.18653/v1/2020.acl-srw.14",
    pages = "95--102",
    abstract = "Medical image captioning can reduce the workload of physicians and save time and expense by automatically generating reports. However, current datasets are small and limited, creating additional challenges for researchers. In this study, we propose a feature difference and tag information combined long short-term memory (LSTM) model for chest x-ray report generation. A feature vector extracted from the image conveys visual information, but its ability to describe the image is limited. Other image captioning studies exhibited improved performance by exploiting feature differences, so the proposed model also utilizes them. First, we propose a difference and tag (DiTag) model containing the difference between the patient and normal images. Then, we propose a multi-difference and tag (mDiTag) model that also contains information about low-level differences, such as contrast, texture, and localized area. Evaluation of the proposed models demonstrates that the mDiTag model provides more information to generate captions and outperforms all other models.",
}

@inproceedings{zhou17-what-you-just-said,
author = {Zhou, Luowei and Xu, Chenliang and Koch, Parker and Corso, Jason J.},
title = {Watch What You Just Said: Image Captioning with Text-Conditional Attention},
year = {2017},
isbn = {9781450354165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126686.3126717},
doi = {10.1145/3126686.3126717},
abstract = {Attention mechanisms have attracted considerable interest in image captioning due to their powerful performance. However, existing methods use only visual content as attention and whether textual context can improve attention in image captioning remains unsolved. To explore this problem, we propose a novel attention mechanism, called text-conditional attention, which allows the caption generator to focus on certain image features given previously generated text. To obtain text-related image features for our attention model, we adopt the guiding Long Short-Term Memory (gLSTM) captioning architecture with CNN fine-tuning. Our proposed method allows joint learning of the image embedding, text embedding, text-conditional attention and language model with one network architecture in an end-to-end manner. We perform extensive experiments on the MS-COCO dataset. The experimental results show that our method outperforms state-of-the-art captioning methods on various quantitative metrics as well as in human evaluation, which supports the use of our text-conditional attention in image captioning.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017},
pages = {305–313},
numpages = {9},
keywords = {lstm, neural network, multi-modal embedding, image captioning},
location = {Mountain View, California, USA},
series = {Thematic Workshops '17}
}

@misc{anderson2017bottomup,
    title={Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
    author={Peter Anderson and Xiaodong He and Chris Buehler and Damien Teney and Mark Johnson and Stephen Gould and Lei Zhang},
    year={2017},
    eprint={1707.07998},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{jhamtani2018learning,
    title={Learning to Describe Differences Between Pairs of Similar Images},
    author={Harsh Jhamtani and Taylor Berg-Kirkpatrick},
    year={2018},
    eprint={1808.10584},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{tan2019expressing,
    title={Expressing Visual Relationships via Language},
    author={Hao Tan and Franck Dernoncourt and Zhe Lin and Trung Bui and Mohit Bansal},
    year={2019},
    eprint={1906.07689},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{forbes2019neural,
    title={Neural Naturalist: Generating Fine-Grained Image Comparisons},
    author={Maxwell Forbes and Christine Kaeser-Chen and Piyush Sharma and Serge Belongie},
    year={2019},
    eprint={1909.04101},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


@misc{darlow2019what,
title={What Information Does a ResNet Compress?},
author={Luke Nicholas Darlow and Amos Storkey},
year={2019},
url={https://openreview.net/forum?id=HklbTjRcKX},
}

@misc{bau2017network,
    title={Network Dissection: Quantifying Interpretability of Deep Visual Representations},
    author={David Bau and Bolei Zhou and Aditya Khosla and Aude Oliva and Antonio Torralba},
    year={2017},
    eprint={1704.05796},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{zhou2017interpreting,
    title={Interpreting Deep Visual Representations via Network Dissection},
    author={Bolei Zhou and David Bau and Aude Oliva and Antonio Torralba},
    year={2017},
    eprint={1711.05611},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{ba2014multiple,
    title={Multiple Object Recognition with Visual Attention},
    author={Jimmy Ba and Volodymyr Mnih and Koray Kavukcuoglu},
    year={2014},
    eprint={1412.7755},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@Article{IUX-ray,
   Author="Demner-Fushman, D.  and Kohli, M. D.  and Rosenman, M. B.  and Shooshan, S. E.  and Rodriguez, L.  and Antani, S.  and Thoma, G. R.  and McDonald, C. J. ",
   Title="{{P}reparing a collection of radiology examinations for distribution and retrieval}",
   Journal="J Am Med Inform Assoc",
   Year="2016",
   Volume="23",
   Number="2",
   Pages="304--310",
   Month="Mar",
   Note={[PubMed Central:\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5009925}{PMC5009925}] [DOI:\href{https://dx.doi.org/10.1093/jamia/ocv080}{10.1093/jamia/ocv080}] [PubMed:\href{https://www.ncbi.nlm.nih.gov/pubmed/24108713}{24108713}] }
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{meteor,
    title = "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3348",
    doi = "10.3115/v1/W14-3348",
    pages = "376--380",
}

@inproceedings{rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@misc{cider,
    title={CIDEr: Consensus-based Image Description Evaluation},
    author={Ramakrishna Vedantam and C. Lawrence Zitnick and Devi Parikh},
    year={2014},
    eprint={1411.5726},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{liu-etal-2015-multi,
    title = "Multi-Timescale Long Short-Term Memory Neural Network for Modelling Sentences and Documents",
    author = "Liu, Pengfei  and
      Qiu, Xipeng  and
      Chen, Xinchi  and
      Wu, Shiyu  and
      Huang, Xuanjing",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1280",
    doi = "10.18653/v1/D15-1280",
    pages = "2326--2335",
}

@inproceedings{martin2018parallelizing,
title={Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
author={Eric Martin and Chris Cundy},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HyUNwulC-},
}

@misc{kougia2019survey,
    title={A Survey on Biomedical Image Captioning},
    author={Vasiliki Kougia and John Pavlopoulos and Ion Androutsopoulos},
    year={2019},
    eprint={1905.13302},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{johnson2019mimiccxrjpg,
    title={MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs},
    author={Alistair E. W. Johnson and Tom J. Pollard and Nathaniel R. Greenbaum and Matthew P. Lungren and Chih-ying Deng and Yifan Peng and Zhiyong Lu and Roger G. Mark and Seth J. Berkowitz and Steven Horng},
    year={2019},
    eprint={1901.07042},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{sarrouti-etal-2020-visual,
    title = "Visual Question Generation from Radiology Images",
    author = "Sarrouti, Mourad  and
      Ben Abacha, Asma  and
      Demner-Fushman, Dina",
    booktitle = "Proceedings of the First Workshop on Advances in Language and Vision Research",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.alvr-1.3",
    doi = "10.18653/v1/2020.alvr-1.3",
    pages = "12--18",
    abstract = "Visual Question Generation (VQG), the task of generating a question based on image contents, is an increasingly important area that combines natural language processing and computer vision. Although there are some recent works that have attempted to generate questions from images in the open domain, the task of VQG in the medical domain has not been explored so far. In this paper, we introduce an approach to generation of visual questions about radiology images called VQGR, i.e. an algorithm that is able to ask a question when shown an image. VQGR first generates new training data from the existing examples, based on contextual word embeddings and image augmentation techniques. It then uses the variational auto-encoders model to encode images into a latent space and decode natural language questions. Experimental automatic evaluations performed on the VQA-RAD dataset of clinical visual questions show that VQGR achieves good performances compared with the baseline system. The source code is available at https://github.com/sarrouti/vqgr.",
}



